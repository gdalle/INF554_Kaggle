{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2015/guillaume.dalle/miniconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# External imports\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import scipy.stats as st\n",
    "from sklearn import linear_model, tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal imports\n",
    "import importlib\n",
    "import extraction as ex\n",
    "import features as feat\n",
    "ex = importlib.reload(ex)\n",
    "feat = importlib.reload(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "READING TRAIN\n",
      "\n",
      "\n",
      "Memory usage (MB) : 8.33381652832\n",
      "Index       7.407837\n",
      "is_churn    0.925980\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "READING TEST\n",
      "\n",
      "\n",
      "Memory usage (MB) : 6.92345428467\n",
      "Index    6.923454\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "READING MEMBERS\n",
      "\n",
      "\n",
      "Memory usage (MB) : 135.57332325\n",
      "Index                     51.646980\n",
      "city                       6.455873\n",
      "bd                        12.911745\n",
      "gender                     6.455873\n",
      "registered_via             6.455873\n",
      "registration_init_time    51.646980\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "# Small tables\n",
    "train = ex.read_train()\n",
    "test = ex.read_test()\n",
    "members = ex.read_members()\n",
    "useful_msno = set.union(\n",
    "    set(train.index.unique()),\n",
    "    set(test.index.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big tables\n",
    "transactions = ex.read_transactions(useful_msno=useful_msno, max_lines=np.inf, chunksize=10**6)\n",
    "user_logs = ex.read_user_logs(useful_msno=useful_msno, just_date=False, max_lines=np.inf, chunksize=10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train set, pretend we don't know what happens in March\n",
    "transactions_train = transactions[transactions[\"transaction_date\"] < pd.Timestamp(2017, 3, 1)]\n",
    "user_logs_train = user_logs[user_logs[\"date\"] < pd.Timestamp(2017, 3, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLOIT MEMBERS\n",
    "\n",
    "def exploit_members(members):\n",
    "    \n",
    "    # Registration init\n",
    "    registration_init = feat.count_days(members[\"registration_init_time\"], base_date=pd.Timestamp(2000, 1, 1))\n",
    "\n",
    "    # Sum up members data\n",
    "    members_data = pd.DataFrame(index=members.index)\n",
    "    members_data[\"registration_init_time\"] = registration_init\n",
    "    \n",
    "    return members_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLOIT TRANSACTIONS\n",
    "\n",
    "def exploit_transactions(transactions, dataset=\"train\"):\n",
    "\n",
    "    # Grouping transactions\n",
    "    grouped_trans = transactions.groupby(\"msno\")\n",
    "    # Groupby indices\n",
    "    trans_msno = grouped_trans.max().index\n",
    "\n",
    "    # Latest transaction line for each user\n",
    "    last_trans_indices = (transactions['transaction_date'] == grouped_trans['transaction_date'].transform(max))\n",
    "    last_transactions = transactions[last_trans_indices].drop_duplicates(\"msno\")\n",
    "    last_transactions.index = last_transactions[\"msno\"]\n",
    "    # Reindex with the same indices as the groupby\n",
    "    last_transactions = last_transactions.reindex(trans_msno).drop(\"msno\", axis=1)\n",
    "    \n",
    "    # Latest transaction\n",
    "    last_trans_date = feat.count_days(grouped_trans[\"transaction_date\"].max(), dataset)\n",
    "    # Planned expiration\n",
    "    last_expiration = feat.count_days(grouped_trans[\"membership_expire_date\"].max(), dataset)\n",
    "    # Number of transactions\n",
    "    count_trans = grouped_trans[\"transaction_date\"].count()\n",
    "\n",
    "    # Mean plan days\n",
    "    mean_plan_days = grouped_trans[\"payment_plan_days\"].mean()\n",
    "    # Total plan days\n",
    "    total_plan_days = grouped_trans[\"payment_plan_days\"].sum()\n",
    "    # Last plan days\n",
    "    last_plan_days = last_transactions[\"payment_plan_days\"]\n",
    "\n",
    "    # Freq auto-renew\n",
    "    freq_auto_renew = grouped_trans[\"is_auto_renew\"].mean()\n",
    "    # Last auto-renew\n",
    "    last_auto_renew = last_transactions[\"is_auto_renew\"]\n",
    "\n",
    "    # Freq cancel\n",
    "    freq_cancel = grouped_trans[\"is_cancel\"].mean()\n",
    "    # Last cancel\n",
    "    last_cancel = last_transactions[\"is_cancel\"]\n",
    "    # Exist cancel\n",
    "    exist_cancel = (grouped_trans[\"is_cancel\"].min() > 0).astype(np.int8)\n",
    "\n",
    "    # Mean price\n",
    "    mean_price = grouped_trans[\"actual_amount_paid\"].sum()\n",
    "    # Total price\n",
    "    total_price = grouped_trans[\"actual_amount_paid\"].sum()\n",
    "\n",
    "    # Majority payment method\n",
    "    # majority_payment_method = grouped_trans[\"payment_method_id\"].agg(lambda x:x.value_counts().index[0])\n",
    "    # Last payment method\n",
    "    last_payment_method = last_transactions[\"payment_method_id\"]\n",
    "\n",
    "    transactions_data = pd.DataFrame(index=trans_msno)\n",
    "    transactions_data[\"Last_transaction_date\"] = last_trans_date\n",
    "    transactions_data[\"Planned_membership_expiration\"] = last_expiration\n",
    "    transactions_data[\"Count_transactions\"] = count_trans\n",
    "    transactions_data[\"Mean_plan_days\"] = mean_plan_days\n",
    "    transactions_data[\"Total_plan_days\"] = total_plan_days\n",
    "    transactions_data[\"Last_plan_days\"] = last_plan_days\n",
    "    transactions_data[\"Freq_auto_renew\"] = freq_auto_renew\n",
    "    transactions_data[\"Last_auto_renew\"] = last_auto_renew\n",
    "    transactions_data[\"Freq_cancel\"] = freq_cancel\n",
    "    transactions_data[\"Last_cancel\"] = last_cancel\n",
    "    transactions_data[\"Exist_cancel\"] = exist_cancel\n",
    "    transactions_data[\"Total_price\"] = total_price\n",
    "    # transactions_data[\"Majority_payment_method\"] = majority_payment_method\n",
    "    transactions_data[\"Last_payment_method\"] = last_payment_method\n",
    "    transactions_data = feat.categorize(transactions_data, \"Last_payment_method\")\n",
    "    \n",
    "    return transactions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLOIT USER LOGS\n",
    "\n",
    "def exploit_user_logs(user_logs, dataset=\"train\"):\n",
    "    \n",
    "    if dataset == \"train\":\n",
    "        user_logs_last_month = user_logs[\n",
    "            (user_logs[\"date\"] > pd.Timestamp(2017, 2, 1)) &\n",
    "            (user_logs[\"date\"] < pd.Timestamp(2017, 3, 1))\n",
    "        ]\n",
    "    elif dataset == \"test\":\n",
    "        user_logs_last_month = user_logs[\n",
    "            (user_logs[\"date\"] > pd.Timestamp(2017, 3, 1)) &\n",
    "            (user_logs[\"date\"] < pd.Timestamp(2017, 4, 1))\n",
    "        ]\n",
    "    \n",
    "    # Grouping user logs\n",
    "    grouped_logs = user_logs.groupby(\"msno\")\n",
    "    grouped_logs_last_month = user_logs_last_month.groupby(\"msno\")\n",
    "    # Storing indices\n",
    "    logs_msno = grouped_logs.max().index\n",
    "\n",
    "    # First log\n",
    "    first_log = feat.count_days(grouped_logs[\"date\"].min(), dataset).fillna(-np.inf)\n",
    "    # Last log\n",
    "    last_log = feat.count_days(grouped_logs[\"date\"].max(), dataset).fillna(-np.inf)\n",
    "\n",
    "    # Number of logs\n",
    "    total_logs = grouped_logs[\"date\"].count().fillna(0)\n",
    "    total_logs_last_month = grouped_logs_last_month[\"date\"].count().fillna(0)\n",
    "    \n",
    "    if len(user_logs.columns) > 2:\n",
    "        # Total unique songs\n",
    "        total_unique_songs = grouped_logs[\"num_unq\"].sum().fillna(0)\n",
    "        total_unique_songs_last_month = grouped_logs_last_month[\"num_unq\"].sum().fillna(0)\n",
    "        # Total 100% songs\n",
    "        total_100_songs = grouped_logs[\"num_100\"].sum().fillna(0)\n",
    "        total_100_songs_last_month = grouped_logs_last_month[\"num_100\"].sum().fillna(0)\n",
    "        # Total seconds\n",
    "        total_seconds = grouped_logs[\"total_secs\"].sum().fillna(0.)\n",
    "        total_seconds_last_month = grouped_logs_last_month[\"total_secs\"].sum().fillna(0.)\n",
    "\n",
    "    logs_data = pd.DataFrame(index = logs_msno)\n",
    "    logs_data[\"Last_log\"] = last_log\n",
    "    logs_data[\"Total_logs\"] = total_logs\n",
    "    logs_data[\"Total_logs_last_month\"] = total_logs_last_month\n",
    "    \n",
    "    if len(user_logs.columns) > 2:\n",
    "        logs_data[\"Total_unique_songs\"] = total_unique_songs\n",
    "        logs_data[\"Total_unique_songs_last_month\"] = total_unique_songs_last_month\n",
    "        logs_data[\"Total_100_songs\"] = total_100_songs\n",
    "        logs_data[\"Total_100_songs_last_month\"] = total_100_songs_last_month\n",
    "        logs_data[\"Total_seconds\"] = total_seconds\n",
    "        logs_data[\"Total_seconds_last_month\"] = total_seconds_last_month\n",
    "    \n",
    "    return logs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and create features\n",
    "members_data = exploit_members(members)\n",
    "transactions_train_data = exploit_transactions(transactions_train, dataset=\"train\")\n",
    "transactions_data = exploit_transactions(transactions, dataset=\"test\")\n",
    "logs_train_data = exploit_user_logs(user_logs_train, dataset=\"train\")\n",
    "logs_data = exploit_user_logs(user_logs, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members_data.to_csv(\"/tmp/kaggle/junk/members_data.csv\")\n",
    "transactions_train_data.to_csv(\"/tmp/kaggle/junk/transactions_train_data.csv\")\n",
    "transactions_data.to_csv(\"/tmp/kaggle/junk/transactions_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_train_data = pd.read_csv(\n",
    "    \"/tmp/kaggle/junk/transactions_train_data.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "transactions_data = pd.read_csv(\n",
    "    \"/tmp/kaggle/junk/transactions_data.csv\",\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_list_train = [transactions_train_data]\n",
    "data_list = [transactions_data]\n",
    "# For now, memory error in dealing with the user logs\n",
    "# data_list_train.append(logs_train_data)\n",
    "# data_list.append(logs_data)\n",
    "# For now, nothing useful in the members\n",
    "# data_list_train.append(members_data)\n",
    "# data_list.append(members_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data to the train set and test dataframes\n",
    "train_full = train.join(data_list_train, how=\"inner\")\n",
    "test_full = test.join(data_list, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the features we want\n",
    "features = test_full.columns # [c for c in test_full.columns if not \"payment_method\" in c]\n",
    "train_filtered, test_filtered = feat.select_features(train_full, test_full, features)\n",
    "\n",
    "# Normalize the columns\n",
    "train_filtered, test_filtered = feat.normalize_features(train_filtered, test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_score_func(y_true, y_pred):\n",
    "    return log_loss(\n",
    "        y_true, y_pred, labels=[0, 1],\n",
    "        eps=np.power(10., -15), normalize=True)\n",
    "\n",
    "log_loss_scorer = make_scorer(\n",
    "    score_func=log_loss_score_func,\n",
    "    greater_is_better=False,\n",
    "    needs_proba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes the machine learning\n",
    "\n",
    "# Conversion into arrays for scikit-learn\n",
    "x = np.array(train_filtered.drop(\"is_churn\", axis=1))\n",
    "y = np.array(train_filtered[\"is_churn\"])\n",
    "xt = np.array(test_filtered)\n",
    "\n",
    "# Train a logistic regression\n",
    "xgbclf = xgb.XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.2)\n",
    "\n",
    "xgbclf.fit(x, y, eval_metric=\"logloss\")\n",
    "yt = xgbclf.predict_proba(xt)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation log loss : 0.194245291633\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "kf = KFold(n_splits=3)\n",
    "scores = []\n",
    "for ind_train, ind_test in kf.split(x):\n",
    "    clf.fit(x[ind_train], y[ind_train], eval_metric=\"logloss\")\n",
    "    y_pred = clf.predict_proba(x[ind_test])\n",
    "    scores.append(log_loss_score_func(y[ind_test], y_pred))\n",
    "print(\"Cross validation log loss :\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks for grid search : 1 \n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] learning_rate=0.0316227766017, max_depth=3, n_estimators=50 .....\n",
      "[CV]  learning_rate=0.0316227766017, max_depth=3, n_estimators=50, total=   7.1s\n",
      "[CV] learning_rate=0.0316227766017, max_depth=3, n_estimators=50 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.5s remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "x = np.array(train_filtered.drop(\"is_churn\", axis=1))\n",
    "y = np.array(train_filtered[\"is_churn\"])\n",
    "xt = np.array(test_filtered)\n",
    "\n",
    "xgbclf = xgb.XGBClassifier()\n",
    "\n",
    "k = 1\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.round(0.5 * np.power(10, np.linspace(2, 3, k))).astype(int),\n",
    "    \"max_depth\": np.arange(3, 10, (10-3) // k),\n",
    "    \"learning_rate\": np.power(10, np.linspace(-1.5, -0.2, k))\n",
    "}\n",
    "\n",
    "total_tasks=1\n",
    "for k in param_grid:\n",
    "    total_tasks *= len(param_grid[k])\n",
    "print(\"Total tasks for grid search :\", total_tasks, \"\\n\")\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=xgbclf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=log_loss_scorer,\n",
    "    n_jobs=1, verbose=3)\n",
    "\n",
    "gs.fit(x, y, eval_metric=\"logloss\")  \n",
    "best_xgbclf = gs.best_estimator_\n",
    "\n",
    "pk.dump(gs.cv_results_, open(\"GS_results\", \"wb\"))\n",
    "pk.dump(gs.best_estimator_, open(\"GS_estimator\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best classifier for prediction\n",
    "best_xgbclf.fit(x, y, eval_metric=\"logloss\")\n",
    "yt = best_xgbclf.predict_proba(xt)[:, 1]\n",
    "\n",
    "# Zero prediction as baseline\n",
    "percentage_churn = train_filtered[\"is_churn\"].sum() / len(train_filtered)\n",
    "test[\"is_churn\"] = np.random.rand(len(test)) * percentage_churn\n",
    "# For users on which we have more info, use it\n",
    "test.loc[test_filtered.index, [\"is_churn\"]] = yt.reshape(-1, 1)\n",
    "\n",
    "# Save as csv\n",
    "submission = test.loc[:, [\"is_churn\"]]\n",
    "submission.to_csv(\"data/submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

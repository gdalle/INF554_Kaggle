{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import itertools\n",
    "import pickle as pk\n",
    "\n",
    "# Machine learning\n",
    "import xgboost as xgb\n",
    "from sklearn import linear_model, tree\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal imports\n",
    "import importlib\n",
    "import extraction as ex\n",
    "import features as feat\n",
    "ex = importlib.reload(ex)\n",
    "feat = importlib.reload(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "# Small tables\n",
    "train = ex.read_train()\n",
    "test = ex.read_test()\n",
    "members = ex.read_members()\n",
    "useful_msno = set.union(\n",
    "    set(train.index.unique()),\n",
    "    set(test.index.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions table\n",
    "transactions = ex.read_transactions(useful_msno=useful_msno, max_lines=10**7, chunk_size=10**6)\n",
    "# For train set, pretend we don't know what happens in March\n",
    "transactions_train = transactions[transactions[\"transaction_date\"] < pd.Timestamp(2017, 3, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLOIT MEMBERS\n",
    "\n",
    "def exploit_members(members):\n",
    "    \n",
    "    # Registration init\n",
    "    registration_init = feat.count_days(members[\"registration_init_time\"], base_date=pd.Timestamp(2000, 1, 1))\n",
    "\n",
    "    # Sum up members features\n",
    "    members_features = pd.DataFrame(index=members.index)\n",
    "    members_features[\"registration_init_time\"] = registration_init\n",
    "    \n",
    "    return members_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLOIT TRANSACTIONS\n",
    "\n",
    "def exploit_transactions(transactions, dataset=\"train\"):\n",
    "\n",
    "    # Grouping transactions\n",
    "    grouped_trans = transactions.groupby(\"msno\")\n",
    "    # Groupby indices\n",
    "    trans_msno = grouped_trans.max().index\n",
    "\n",
    "    # Latest transaction line for each user\n",
    "    last_trans_indices = (transactions['transaction_date'] == grouped_trans['transaction_date'].transform(max))\n",
    "    last_transactions = transactions[last_trans_indices].drop_duplicates(\"msno\")\n",
    "    last_transactions.index = last_transactions[\"msno\"]\n",
    "    # Reindex with the same indices as the groupby\n",
    "    last_transactions = last_transactions.reindex(trans_msno).drop(\"msno\", axis=1)\n",
    "    \n",
    "    # Latest transaction\n",
    "    last_trans_date = feat.count_days(grouped_trans[\"transaction_date\"].max(), dataset)\n",
    "    # Planned expiration\n",
    "    last_expiration = feat.count_days(grouped_trans[\"membership_expire_date\"].max(), dataset)\n",
    "    # Number of transactions\n",
    "    count_trans = grouped_trans[\"transaction_date\"].count()\n",
    "\n",
    "    # Mean plan days\n",
    "    mean_plan_days = grouped_trans[\"payment_plan_days\"].mean()\n",
    "    # Total plan days\n",
    "    total_plan_days = grouped_trans[\"payment_plan_days\"].sum()\n",
    "    # Last plan days\n",
    "    last_plan_days = last_transactions[\"payment_plan_days\"]\n",
    "\n",
    "    # Freq auto-renew\n",
    "    freq_auto_renew = grouped_trans[\"is_auto_renew\"].mean()\n",
    "    # Last auto-renew\n",
    "    last_auto_renew = last_transactions[\"is_auto_renew\"]\n",
    "\n",
    "    # Freq cancel\n",
    "    freq_cancel = grouped_trans[\"is_cancel\"].mean()\n",
    "    # Last cancel\n",
    "    last_cancel = last_transactions[\"is_cancel\"]\n",
    "    # Exist cancel\n",
    "    exist_cancel = (grouped_trans[\"is_cancel\"].min() > 0).astype(np.int8)\n",
    "\n",
    "    # Mean price\n",
    "    mean_price = grouped_trans[\"actual_amount_paid\"].sum()\n",
    "    # Total price\n",
    "    total_price = grouped_trans[\"actual_amount_paid\"].sum()\n",
    "\n",
    "    # Majority payment method\n",
    "    # majority_payment_method = grouped_trans[\"payment_method_id\"].agg(lambda x:x.value_counts().index[0])\n",
    "    # Last payment method\n",
    "    last_payment_method = last_transactions[\"payment_method_id\"]\n",
    "\n",
    "    # Add all of this to the features table\n",
    "    transactions_features = pd.DataFrame(index=trans_msno)\n",
    "    transactions_features[\"Last_transaction_date\"] = last_trans_date\n",
    "    transactions_features[\"Planned_membership_expiration\"] = last_expiration\n",
    "    transactions_features[\"Count_transactions\"] = count_trans\n",
    "    transactions_features[\"Mean_plan_days\"] = mean_plan_days\n",
    "    transactions_features[\"Total_plan_days\"] = total_plan_days\n",
    "    transactions_features[\"Last_plan_days\"] = last_plan_days\n",
    "    transactions_features[\"Freq_auto_renew\"] = freq_auto_renew\n",
    "    transactions_features[\"Last_auto_renew\"] = last_auto_renew\n",
    "    transactions_features[\"Freq_cancel\"] = freq_cancel\n",
    "    transactions_features[\"Last_cancel\"] = last_cancel\n",
    "    transactions_features[\"Exist_cancel\"] = exist_cancel\n",
    "    transactions_features[\"Total_price\"] = total_price\n",
    "    # transactions_features[\"Majority_payment_method\"] = majority_payment_method\n",
    "    transactions_features[\"Last_payment_method\"] = last_payment_method\n",
    "    transactions_features = feat.categorize(transactions_features, \"Last_payment_method\")\n",
    "    \n",
    "    return transactions_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLOIT USER LOGS\n",
    "\n",
    "def exploit_user_logs(user_logs, dataset=\"train\"):\n",
    "    \n",
    "    if dataset == \"train\":\n",
    "        user_logs_last_month = user_logs[\n",
    "            (user_logs[\"date\"] > pd.Timestamp(2017, 2, 1)) &\n",
    "            (user_logs[\"date\"] < pd.Timestamp(2017, 3, 1))\n",
    "        ]\n",
    "    elif dataset == \"test\":\n",
    "        user_logs_last_month = user_logs[\n",
    "            (user_logs[\"date\"] > pd.Timestamp(2017, 3, 1)) &\n",
    "            (user_logs[\"date\"] < pd.Timestamp(2017, 4, 1))\n",
    "        ]\n",
    "    \n",
    "    # Grouping user logs\n",
    "    grouped_logs = user_logs.groupby(\"msno\")\n",
    "    grouped_logs_last_month = user_logs_last_month.groupby(\"msno\")\n",
    "    # Storing indices\n",
    "    logs_msno = grouped_logs.max().index\n",
    "\n",
    "    # First log\n",
    "    first_log = feat.count_days(grouped_logs[\"date\"].min(), dataset)\n",
    "    # Last log\n",
    "    last_log = feat.count_days(grouped_logs[\"date\"].max(), dataset)\n",
    "\n",
    "    # Number of logs\n",
    "    total_logs = grouped_logs[\"date\"].count()\n",
    "    total_logs_last_month = grouped_logs_last_month[\"date\"].count()\n",
    "\n",
    "    logs_features = pd.DataFrame(index = logs_msno)\n",
    "    logs_features[\"Last_log\"] = last_log\n",
    "    logs_features[\"First_log\"] = first_log\n",
    "    logs_features[\"Total_logs\"] = total_logs\n",
    "    logs_features[\"Total_logs_last_month\"] = total_logs_last_month\n",
    "    \n",
    "    if len(user_logs.columns) > 2:\n",
    "        \n",
    "        # Total unique songs\n",
    "        total_unique_songs = grouped_logs[\"num_unq\"].sum()\n",
    "        total_unique_songs_last_month = grouped_logs_last_month[\"num_unq\"].sum()\n",
    "        # Total 100% songs\n",
    "        total_100_songs = grouped_logs[\"num_100\"].sum()\n",
    "        total_100_songs_last_month = grouped_logs_last_month[\"num_100\"].sum()\n",
    "        # Total seconds\n",
    "        total_seconds = grouped_logs[\"total_secs\"].sum()\n",
    "        total_seconds_last_month = grouped_logs_last_month[\"total_secs\"].sum()\n",
    "\n",
    "        logs_features[\"Total_unique_songs\"] = total_unique_songs\n",
    "        logs_features[\"Total_unique_songs_last_month\"] = total_unique_songs_last_month\n",
    "        logs_features[\"Total_100_songs\"] = total_100_songs\n",
    "        logs_features[\"Total_100_songs_last_month\"] = total_100_songs_last_month\n",
    "        logs_features[\"Total_seconds\"] = total_seconds\n",
    "        logs_features[\"Total_seconds_last_month\"] = total_seconds_last_month\n",
    "    \n",
    "    return logs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunk_to_logs_features(logs_chunk, logs_features, max_lag):\n",
    "    \"\"\"Extract features from a chunk of the user_logs and save them.\"\"\"\n",
    "    if len(logs_chunk) == 0:\n",
    "        return\n",
    "    # Collect features on the chunk\n",
    "    logs_chunk_features = exploit_user_logs(logs_chunk)\n",
    "    # Adjust the index on that of all useful users\n",
    "    # (introducing NaN for the ones not present in the chunk)\n",
    "    logs_chunk_features = logs_chunk_features.reindex(useful_msno)\n",
    "\n",
    "    logs_features[\"Last_log\"] = np.minimum(\n",
    "        logs_features[\"Last_log\"],\n",
    "        logs_chunk_features[\"Last_log\"].fillna(max_lag))\n",
    "    logs_features[\"First_log\"] = np.maximum(\n",
    "        logs_features[\"First_log\"],\n",
    "        logs_chunk_features[\"Last_log\"].fillna(-max_lag))\n",
    "    logs_features[\"Total_logs\"] += logs_chunk_features[\"Total_logs\"].fillna(0)\n",
    "    logs_features[\"Total_logs_last_month\"] += logs_chunk_features[\"Total_logs_last_month\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_user_logs_features(\n",
    "    useful_msno, dataset,\n",
    "    starting_date=None, just_date=False,\n",
    "    max_lines=np.inf, chunk_size=10**6\n",
    "):\n",
    "    \"\"\"Read user logs, extract features on the fly.\"\"\"\n",
    "    print(\"\\nREADING USER LOGS WITH FEATURE EXTRACTION FOR TRAIN & TEST\\n\")\n",
    "    \n",
    "    global_path = \"/tmp/kaggle/proj2/\"\n",
    "    \n",
    "    logs_features = pd.DataFrame(index=useful_msno)\n",
    "    logs_train_features = pd.DataFrame(index=useful_msno)\n",
    "    \n",
    "    max_lag = 10000\n",
    "    for df in [logs_features, logs_train_features]:\n",
    "        # Initialize values\n",
    "        df[\"Total_logs\"] = 0\n",
    "        df[\"Total_logs_last_month\"] = 0\n",
    "        # Days counted backwards from today\n",
    "        df[\"Last_log\"] = max_lag\n",
    "        df[\"First_log\"] = -max_lag\n",
    "\n",
    "    dtype_cols_user_logs = {\n",
    "        'msno': object,\n",
    "        'date': np.int64,\n",
    "        'num_25': np.int8,\n",
    "        'num_50': np.int8,\n",
    "        'num_75': np.int8,\n",
    "        'num_985': np.int8,\n",
    "        'num_100': np.int8,\n",
    "        'num_unq': np.int8,\n",
    "        'total_secs': np.float32\n",
    "    }\n",
    "\n",
    "    if just_date:\n",
    "        usecols=[\"date\", \"msno\"]\n",
    "    else:\n",
    "        usecols=[\"date\", \"msno\", \"num_100\", \"num_unq\", \"total_secs\"]\n",
    "\n",
    "    # Read all columns\n",
    "    iterator1 = pd.read_csv(\n",
    "        global_path + \"data/user_logs.csv\",\n",
    "        chunksize=chunk_size,\n",
    "        iterator=True,\n",
    "        header=0,\n",
    "        dtype=dtype_cols_user_logs,\n",
    "        usecols=usecols\n",
    "    )\n",
    "    iterator2 = pd.read_csv(\n",
    "        global_path + \"data/user_logs_v2.csv\",\n",
    "        chunksize=chunk_size,\n",
    "        iterator=True,\n",
    "        header=0,\n",
    "        dtype=dtype_cols_user_logs,\n",
    "        usecols=usecols\n",
    "    )\n",
    "\n",
    "    # Read data by chunks to alleviate memory load\n",
    "    chunk_number = 0\n",
    "    for logs_chunk in itertools.chain(iterator1, iterator2):\n",
    "\n",
    "        # Convert dates to pandas format\n",
    "        logs_chunk[\"date\"] = pd.to_datetime(logs_chunk[\"date\"].astype(str))\n",
    "        # Filter dates for train set\n",
    "        logs_chunk_train = logs_chunk[logs_chunk[\"date\"] < pd.Timestamp(2017, 3, 1)]\n",
    "        \n",
    "        # Add the new information from this chunk to both features tables\n",
    "        add_chunk_to_logs_features(logs_chunk, logs_features, max_lag)\n",
    "        add_chunk_to_logs_features(logs_chunk_train, logs_train_features, max_lag)\n",
    "        \n",
    "        print(\"Chunk {} of user logs read\".format(chunk_number + 1))\n",
    "        chunk_number += 1\n",
    "        if chunk_number >= max_lines / chunk_size:\n",
    "            break\n",
    "        \n",
    "    # Get rid of users for which we haven't read anything\n",
    "    logs_features = logs_features.replace([max_lag, -max_lag], np.NaN).dropna()\n",
    "    logs_train_features = logs_train_features.replace([max_lag, -max_lag], np.NaN).dropna()\n",
    "\n",
    "    return logs_train_features, logs_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from preexisting tables\n",
    "members_features = exploit_members(members)\n",
    "transactions_train_features = exploit_transactions(transactions_train, dataset=\"train\")\n",
    "transactions_features = exploit_transactions(transactions, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract features from user_logs while reading it\n",
    "logs_train_features, logs_features = read_user_logs_features(\n",
    "    useful_msno=useful_msno, dataset=\"train\",\n",
    "    starting_date=None, just_date=False,\n",
    "    max_lines=10**7, chunk_size=10**6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features to csv to get them back easily\n",
    "members_features.to_csv(\"/tmp/kaggle/proj2/junk/members_features.csv\")\n",
    "transactions_train_features.to_csv(\"/tmp/kaggle/proj2/junk/transactions_train_features.csv\")\n",
    "transactions_features.to_csv(\"/tmp/kaggle/proj2/junk/transactions_features.csv\")\n",
    "logs_train_features.to_csv(\"/tmp/kaggle/proj2/junk/logs_train_features.csv\")\n",
    "logs_features.to_csv(\"/tmp/kaggle/proj2/junk/logs_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members_features = pd.read_csv(\n",
    "    \"/tmp/kaggle/proj2/junk/members_features.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "transactions_train_features = pd.read_csv(\n",
    "    \"/tmp/kaggle/proj2/junk/transactions_train_features.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "transactions_features = pd.read_csv(\n",
    "    \"/tmp/kaggle/proj2/junk/transactions_features.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "logs_train_features = pd.read_csv(\n",
    "    \"/tmp/kaggle/proj2/junk/logs_train_features.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "logs_features = pd.read_csv(\n",
    "    \"/tmp/kaggle/proj2/junk/logs_features.csv\",\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data to the train set and test dataframes\n",
    "features_list_train = [members_features, transactions_train_features, logs_train_features]\n",
    "features_list = [members_features, transactions_features, logs_train_features]\n",
    "# The inner join keeps only the users present in every table\n",
    "train_full = train.join(features_list_train, how=\"inner\")\n",
    "test_full = test.join(features_list, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the features we want\n",
    "features = test_full.columns # [c for c in test_full.columns if not \"payment_method\" in c]\n",
    "train_filtered, test_filtered = feat.select_features(train_full, test_full, features)\n",
    "\n",
    "# Normalize the columns\n",
    "train_filtered, test_filtered = feat.normalize_features(train_filtered, test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_score_func(y_true, y_pred):\n",
    "    return log_loss(\n",
    "        y_true, y_pred, labels=[0, 1],\n",
    "        eps=np.power(10., -15), normalize=True)\n",
    "\n",
    "log_loss_scorer = make_scorer(\n",
    "    score_func=log_loss_score_func,\n",
    "    greater_is_better=False,\n",
    "    needs_proba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here comes the machine learning\n",
    "\n",
    "# Conversion into arrays for scikit-learn\n",
    "x = np.array(train_filtered.drop(\"is_churn\", axis=1))\n",
    "y = np.array(train_filtered[\"is_churn\"])\n",
    "xt = np.array(test_filtered)\n",
    "\n",
    "# Initialize xgboost (with arbitrary parameters for now)\n",
    "xgbclf = xgb.XGBClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(\n",
    "    xgbclf, x, y,\n",
    "    scoring=\"neg_log_loss\",\n",
    "    cv=3, n_jobs=1,\n",
    "    verbose=3,\n",
    "    fit_params={\"eval_metric\": \"logloss\"}\n",
    ")\n",
    "print(\"Cross-validation score\", np.mean(scores.mean())\n",
    "\n",
    "# Train the classifier on all of the data\n",
    "xgbclf.fit(x, y, eval_metric=\"logloss\")\n",
    "# Perform the prediction\n",
    "yt = xgbclf.predict_proba(xt)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter selection for xgboost\n",
    "\n",
    "x = np.array(train_filtered.drop(\"is_churn\", axis=1))\n",
    "y = np.array(train_filtered[\"is_churn\"])\n",
    "xt = np.array(test_filtered)\n",
    "\n",
    "xgbclf = xgb.XGBClassifier()\n",
    "\n",
    "k = 3\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.round(0.5 * np.power(10, np.linspace(2, 3, k))).astype(int),\n",
    "    \"max_depth\": np.arange(3, 10, (10-3) // k),\n",
    "    \"learning_rate\": np.power(10, np.linspace(-1.5, -0.2, k))\n",
    "}\n",
    "\n",
    "total_tasks=1\n",
    "for k in param_grid:\n",
    "    total_tasks *= len(param_grid[k])\n",
    "print(\"Total tasks for grid search :\", total_tasks, \"\\n\")\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=xgbclf,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    scoring=\"neg_log_loss\",\n",
    "    n_jobs=1, verbose=3)\n",
    "\n",
    "gs.fit(x, y, eval_metric=\"logloss\")  \n",
    "best_xgbclf = gs.best_estimator_\n",
    "\n",
    "# pk.dump(gs.cv_results_, open(\"GS_results\", \"wb\"))\n",
    "# pk.dump(gs.best_estimator_, open(\"GS_estimator\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best classifier for prediction\n",
    "best_xgbclf.fit(x, y, eval_metric=\"logloss\")\n",
    "yt = best_xgbclf.predict_proba(xt)[:, 1]\n",
    "\n",
    "# Zero prediction as baseline\n",
    "percentage_churn = train_filtered[\"is_churn\"].sum() / len(train_filtered)\n",
    "test[\"is_churn\"] = np.random.rand(len(test)) * percentage_churn\n",
    "# For users on which we have more info, use it\n",
    "test.loc[test_filtered.index, [\"is_churn\"]] = yt.reshape(-1, 1)\n",
    "\n",
    "# Save as csv\n",
    "submission = test.loc[:, [\"is_churn\"]]\n",
    "submission.to_csv(\"data/submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
